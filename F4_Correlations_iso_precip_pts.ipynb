{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59ba4829",
   "metadata": {},
   "source": [
    "# Correlations between hydroclimate variables (w500, d18op, precipitation) and IPO, ENSO. from iCAM5 simulations, terrestrial archives, and the Global Precipitation Climatology Centre (GPCC) precipitaiton product\n",
    "\n",
    "Figure 4. Correlations between Pacific SST modes and South American DJF hydroclimate, including a) IPO and ω500 hPa from iCAM5, c) IPO and δ¹⁸Oₚ from iCAM5 and proxy records (plotted as circles), e) IPO and precipitation from iCAM5 and GPCC (plotted as circles); b, d, and f) as in a, c and e) but for ENSO. The core region of the ω500 and δ¹⁸Oₚ responses to ENSO and IPO indices are outlined in dashed boxes. Statistical significance at p<0.05 is indicated with a black cross (proxies) and black stippling (model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303cce6",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68694c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------- LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr   # use xarray to open data files instead of netCDF4.  It is faster, and easier. And, it keeps my metadata!\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.patches as patches\n",
    "import proplot as pplt\n",
    "\n",
    "# ---------------------------- IGNORE PLOTTING WARNING\n",
    "import warnings\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)\n",
    "\n",
    "# # ---------------------------- LOADING FUNCTIONS\n",
    "def geo_loc(target,array):\n",
    "    return array[np.abs(array - target).argmin()]\n",
    "# xr.open_mfdataset('*.nc', combine='by_coords', preprocess=drop_all_coords)\n",
    "\n",
    "def geo_idx(target,array):\n",
    "    return np.abs(array - target).argmin()\n",
    "\n",
    "#statistical significance for smoothed data, decreased degrees of freedom. Henry He\n",
    "def t_p_test(x1, x2, lpp): \n",
    "    \"\"\"\n",
    "    calculate statistical significance of correlation using adjusted degrees of freedom for smoothed data\n",
    "    also calculates the p value for tx1: population sample. \n",
    "    More info on calculating the pvalue with adjusted degrees of freedom here: https://docs.scipy.org/doc/scipy/tutorial/stats.html\n",
    "    x2: population sample\n",
    "    x3: number of smoothing units\n",
    "    llp: number of years in the smoothing. \n",
    "    returns: t statistic and the p value\n",
    "    \"\"\"\n",
    "    n = x1.shape[0]                      # ipo_index_had.shape = 130\n",
    "    df = 2*n/lpp - 2                     # 2*130 / 11 - 2 = 28.889\n",
    "#     nas = np.isnan(x2)\n",
    "#     r = stats.pearsonr(x1[~nas], x2[~nas])[0]        # need same shape: pearsonr(ipo_index_had, var[:,0,1])[0]\n",
    "    r = stats.pearsonr(x1, x2)[0]        # need same shape: pearsonr(ipo_index_had, var[:,0,1])[0]\n",
    "    error = np.sqrt((1 - r**2) / df)     # sqrt needs np.\n",
    "    t = r/error                          # float value\n",
    "    p = stats.t.sf(np.abs(t), df)*2      # calculate p value\n",
    "    return r, p\n",
    "\n",
    "# Detrend time series data for correlations.\n",
    "def detrend_dim(da, dim, deg=1):\n",
    "    # detrend along a single dimension\n",
    "    p = da.polyfit(dim=dim, deg=deg)\n",
    "    fit = xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    return da - (fit - fit.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8bcc0",
   "metadata": {},
   "source": [
    "## Load parameters, process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea3461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- LOADING HYDROCLIMATE VARIABLES for IPO and ENSO\n",
    "path = \"/network/rit/home/ro553136/orrison/data/CESM/iCAM5/cntl/atm/f09/\"\n",
    "fh_w500 = xr.open_dataset(path + \"f.e13.FiPIPDC5CN.f09_f09.ctrl.cam.h0.w500_1850_2013.nc\").sel(lat=slice(-50,15),lon=slice(260,340))\n",
    "fh_d18o_pwt = xr.open_dataset(path + \"f.e13.FiPIPDC5CN.f09_f09.ctrl.cam.h0.d18o_pwt_1850_2013.nc\").sel(lat=slice(-50,15),lon=slice(260,340))\n",
    "fh_p = xr.open_dataset(path + \"f.e13.FiPIPDC5CN.f09_f09.ctrl.cam.h0.prect_1850_2013.nc\").sel(lat=slice(-50,15),lon=slice(260,340))\n",
    "lat = fh_p.lat\n",
    "lon = fh_p.lon\n",
    "nlat = len(lat)\n",
    "nlon = len(lon)\n",
    "\n",
    "# -------------- SUBSET MODEL DATA\n",
    "\n",
    "icam_yrs = np.arange(1880,2000)\n",
    "\n",
    "# model data: [30:-14]: 1880 -- 1999\n",
    "icam_om_djf = fh_w500.w500[30:-14] \n",
    "icam_d18opwtdjf = fh_d18o_pwt.d18o_pwt[30:-14] \n",
    "icam_pdjf = fh_p.prect[30:-14] \n",
    "\n",
    "vars_all = [icam_om_djf, icam_d18opwtdjf, icam_pdjf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9527fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load precip data: \n",
    "# GPCC\n",
    "gpcc_pth = \"/network/rit/home/ro553136/orrison/data/observations/GPCC/\"\n",
    "gpcc_f = \"DJF_v2018_05_seassum.nc\"\n",
    "fh_gpcc = xr.open_dataset(gpcc_pth + gpcc_f)\n",
    "fh_gpcc['lon'] = 360 + fh_gpcc.lon\n",
    "gpcc_pr = fh_gpcc.precip[:110].load()\n",
    "gpcc_pr = gpcc_pr.sel(lat=slice(15,-50),lon=slice(275,325))\n",
    "\n",
    "gpcc_pr['time'] = np.arange(1890,2000)\n",
    "yrs = gpcc_pr.time\n",
    "gpcc_lat = gpcc_pr.lat\n",
    "gpcc_lon = gpcc_pr.lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a9c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- DETREND, SAVE AS XR, APPLY 11 YR SMOOTHING\n",
    "# detrend data, save as xr, smooth\n",
    "vars_all_11yr = []\n",
    "vars_all_dt = []\n",
    "for vari in vars_all:\n",
    "    vari_dt = np.zeros(vari.shape)\n",
    "#------------------- detrending all data -------------------\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            var_tmp = vari[:,i,j]\n",
    "            # skip missing values - lat/lon points where the nas is identified. relevant for d18o, d18opwt\n",
    "            nas = np.isnan(vari[:,i,j])\n",
    "#             vari_dt[:,i,j] = signal.detrend(var_tmp)\n",
    "            m, b, r_val, p_val, std_err = stats.linregress(vari[:,i,j][~nas].time,vari[:,i,j][~nas])\n",
    "            x = vari[:,i,j][~nas].time\n",
    "            trend_line = m*x+b\n",
    "            vari_detrended = vari[:,i,j][~nas] - trend_line\n",
    "            vari_dt[:,i,j] = vari_detrended.reindex(time=icam_yrs,fill_value=np.nan)\n",
    "#------------------- format as an xr Data Array -------------------\n",
    "    var = xr.DataArray(vari_dt, \n",
    "        coords={'time': icam_yrs,'latitude': lat,'longitude': lon}, \n",
    "        dims=[\"time\",\"lat\", \"lon\"], name = vari.name)     \n",
    "#------------------- set up for smoothing -------------------\n",
    "    var_11yr = xr.DataArray(np.zeros([len(vari.time), len(lat), len(lon)]), \n",
    "        coords={'time': icam_yrs,'latitude': lat,'longitude': lon}, \n",
    "        dims=[\"time\",\"lat\", \"lon\"], name = vari.name)               \n",
    "\n",
    "#------------------- 11 year smoothing to match TPI -------------------\n",
    "    nt = len(var.time)\n",
    "    filter_11yr = 11\n",
    "    mirror = int((filter_11yr-1)/2) # end point\n",
    "    copy = np.zeros((nt+2*mirror))\n",
    "    for i in range(len(lat)):\n",
    "        for j in range(len(lon)):\n",
    "            copy[mirror:-mirror] = var[:,i,j].copy()\n",
    "            copy[0:mirror] = var[1:mirror+1,i,j].copy()[::-1]\n",
    "            copy[-mirror:] = var[-1-mirror:-1,i,j].copy()[::-1]\n",
    "            avg_mask_11yr = np.ones(filter_11yr) / np.real(filter_11yr)\n",
    "            var_11yr[:,i,j] = np.convolve(copy, avg_mask_11yr, 'same')[mirror:-mirror]\n",
    "    vars_all_dt.append(var)\n",
    "    vars_all_11yr.append(var_11yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34877b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- FOR GPCC Precipitation\n",
    "# -------------- DETREND, SAVE AS XR, APPLY 11 YR SMOOTHING\n",
    "# detrend data, save as xr, smooth\n",
    "vari_dt = np.zeros(gpcc_pr.shape)\n",
    "for i in range(len(gpcc_lat)):\n",
    "    for j in range(len(gpcc_lon)):\n",
    "        var_tmp = gpcc_pr[:,i,j]#.astype(float)\n",
    "        nas = np.isnan(gpcc_pr[:,i,j])\n",
    "        if not sum(nas) == 110:\n",
    "            m, b, r_val, p_val, std_err = stats.linregress(gpcc_pr[:,i,j][~nas].time,gpcc_pr[:,i,j][~nas])\n",
    "            x = gpcc_pr[:,i,j][~nas].time\n",
    "            trend_line = m*x+b\n",
    "            vari_detrended = gpcc_pr[:,i,j][~nas] - trend_line\n",
    "            vari_dt[:,i,j] = vari_detrended.reindex(time=yrs,fill_value=np.nan)\n",
    "        else: vari_dt[:,i,j] == gpcc_pr[:,i,j]\n",
    "        \n",
    "#------------------- format as an xr Data Array -------------------\n",
    "var = xr.DataArray(vari_dt, \n",
    "    coords={'time': yrs,'latitude': gpcc_lat,'longitude': gpcc_lon}, \n",
    "    dims=[\"time\",\"lat\", \"lon\"], name = 'GPCC Prect')     \n",
    "#------------------- set up for smoothing -------------------\n",
    "var_11yr = xr.DataArray(np.zeros([len(yrs), len(gpcc_lat), len(gpcc_lon)]), \n",
    "    coords={'time': yrs,'latitude': gpcc_lat,'longitude': gpcc_lon}, \n",
    "    dims=[\"time\",\"lat\", \"lon\"], name = 'GPCC Prect')   \n",
    "        \n",
    "#------------------- 11 year smoothing to match TPI -------------------\n",
    "nt = len(var.time)\n",
    "filter_11yr = 11\n",
    "mirror = int((filter_11yr-1)/2) # end point\n",
    "copy = np.zeros((nt+2*mirror))\n",
    "for i in range(len(gpcc_lat)):\n",
    "    for j in range(len(gpcc_lon)):\n",
    "        copy[mirror:-mirror] = var[:,i,j].copy()\n",
    "        copy[0:mirror] = var[1:mirror+1,i,j].copy()[::-1]\n",
    "        copy[-mirror:] = var[-1-mirror:-1,i,j].copy()[::-1]\n",
    "        avg_mask_11yr = np.ones(filter_11yr) / np.real(filter_11yr)\n",
    "        var_11yr[:,i,j] = np.convolve(copy, avg_mask_11yr, 'same')[mirror:-mirror]\n",
    "        \n",
    "vars_all_dt.append(var)\n",
    "vars_all_11yr.append(var_11yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16332ef6",
   "metadata": {},
   "source": [
    "## Load records for the IPO/ENSO analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load proxy data for IPO and ENSO. Time series are detrended.\n",
    "\n",
    "# Run script to generate nice data data frames\n",
    "%run /network/rit/home/ro553136/orrison/Scripts/processing/proxy_recs_fileprocessing_ipo_enso.ipynb\n",
    "\n",
    "    # Make dictionary to store proxy information.\n",
    "recs = ['P00H1', 'JAR4', 'CR1', 'SBE3', 'DV2+TR5+LD12', 'PIM4',#'MFZ',\n",
    "        'Quelccaya', 'Pumacocha','UTU','Cuy','Bol']\n",
    "lat_pts = [-11.27, -21.08, -24.58, -13.81, -12.36,  -11.4,  -24.66, \n",
    "           -13.93, -10.07, -21.7, -0.45, -11.0]\n",
    "lon_pts = [284.21, 304.42, 311.42, 313.65, 318.43, 299.62, 310.46, \n",
    "           289.17, 283.94, 293.23, 281.96, 293.08]\n",
    "    # interpolate huagapo to a lower position\n",
    "lat_pts_nn_p00 = (geo_loc(lat_pts[0],lat) - (lat[1]-lat[0])).values\n",
    "lat_pts[0] = lat_pts_nn_p00\n",
    "# rec_dict = {'modpts': recs,'lat': lat_pts, 'lon': lon_pts}\n",
    "\n",
    " # -------------------------- Load dendro records\n",
    "\n",
    "    # load UTU data onto the mceof pd DF\n",
    "pth_den = \"/network/rit/home/ro553136/orrison/data/proxy/dendro/iso_o/\"\n",
    "f_den = open(pth_den + \"utu18O_1700-2013.robmean.csv\")\n",
    "utu = pd.read_csv(f_den, index_col=0)\n",
    "utu_dat = utu['d18Outu'].iloc[180:-15]\n",
    "utu_pd = pd.Series(utu_dat, index=utu.index[180:-15].values)\n",
    "utu_pd.loc[1996] = 33.93\n",
    "\n",
    "   # load Baker data: Ecuador and Bolivia\n",
    "f_bak = \"Baker_2022_supplemental_data.xlsx\"\n",
    "df_bak = pd.read_excel(open(pth_den + f_bak, 'rb'))\n",
    "bak_yrs = df_bak.iloc[20:140, 0][::-1]\n",
    "cuy_dat = df_bak.iloc[20:140, 1][::-1].astype(float)\n",
    "cuy_pd = pd.Series(cuy_dat.values, index=bak_yrs.values)    \n",
    "bol_dat = df_bak.iloc[20:140, 2][::-1].astype(float)\n",
    "bol_pd = pd.Series(bol_dat.values, index=bak_yrs.values)    \n",
    "\n",
    "# append dataframe with data values\n",
    "abs_prox_hist = abs_prox_hist.assign(UTU=utu_pd)\n",
    "abs_prox_hist = abs_prox_hist.assign(Cuy=cuy_pd)\n",
    "abs_prox_hist = abs_prox_hist.assign(Bol=bol_pd)\n",
    "\n",
    "abs_prox_hist.rename(columns = {'JAR': 'JAR4'}, inplace = True)\n",
    "abs_prox_hist.rename(columns = {'SBE3+SMT5': 'SBE3'}, inplace = True)\n",
    "\n",
    "# Linearly interoplate MFZ to conform to the dataframe\n",
    "mfz_f = interpolate.interp1d(MFZ_pd.index,MFZ_pd, fill_value='extrapolate')\n",
    "mfz_yrann = np.arange(1880,2000) # input values for exact ann. interp of varved non-exact section\n",
    "mfz_int = mfz_f(mfz_yrann)       # interpolated data\n",
    "mfz_pd = pd.Series(mfz_int, index=mfz_yrann)   \n",
    "\n",
    "abs_prox_hist_md = {}\n",
    "for r in recs:\n",
    "    abs_prox_hist_md[r] = pd.Series([abs_prox_hist[r].loc[i].median(axis=0) for i in np.arange(1880,2000)])\n",
    "    \n",
    "abs_prox_over_ipo = pd.DataFrame.from_dict(abs_prox_hist_md)\n",
    "abs_prox_over_ipo.insert(6,'MFZ',mfz_pd.values)\n",
    "abs_prox_over_ipo.index = np.arange(1880,2000)\n",
    "\n",
    "recs = ['P00H1', 'JAR4', 'CR1', 'SBE3', 'DV2+TR5+LD12', 'PIM4','MFZ',\n",
    "        'Quelccaya', 'Pumacocha','UTU','Cuy','Bol']\n",
    "\n",
    " # -------------------------- Subset full dataframe for ENSO\n",
    "recs_enso = recs[6:]\n",
    "# del recs_enso[2]\n",
    "lat_pts_enso = lat_pts[6:]\n",
    "# del lat_pts_enso[2]\n",
    "lon_pts_enso = lon_pts[6:]\n",
    "# del lon_pts_enso[2]\n",
    "abs_prox_over_enso = abs_prox_over_ipo[recs_enso]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d38d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- Smooth Pumacocha, tree ring cellulose records\n",
    "prox_pd = []\n",
    "for i,r in enumerate(recs[-4:]):\n",
    "    var = abs_prox_over_ipo[r]\n",
    "    nt = len(var.index)\n",
    "    filter_11yr = 11\n",
    "    mirror = int((filter_11yr-1)/2) # end point\n",
    "    copy = np.zeros((nt+2*mirror))\n",
    "    copy[mirror:-mirror] = var.copy()\n",
    "    copy[0:mirror] = var[1:mirror+1].copy()[::-1]\n",
    "    copy[-mirror:] = var[-1-mirror:-1].copy()[::-1]\n",
    "    avg_mask_11yr = np.ones(filter_11yr) / np.real(filter_11yr)\n",
    "    var_11yr = np.convolve(copy, avg_mask_11yr, 'same')[mirror:-mirror]\n",
    "    \n",
    "#     convert to pd series\n",
    "    prox_yrann = np.arange(1880,2000) \n",
    "    prox_pd.append(pd.Series(var_11yr, index=prox_yrann))\n",
    "\n",
    "#     replace in the other series\n",
    "abs_prox_over_ipo = abs_prox_over_ipo.assign(Pumacocha=prox_pd[0])\n",
    "abs_prox_over_ipo = abs_prox_over_ipo.assign(UTU=prox_pd[1])\n",
    "abs_prox_over_ipo = abs_prox_over_ipo.assign(Cuy=prox_pd[2])\n",
    "abs_prox_over_ipo = abs_prox_over_ipo.assign(Bol=prox_pd[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801a130",
   "metadata": {},
   "source": [
    "## Load IPO, ENSO indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e95232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------- IPO -------------------\n",
    "pth_hadsst = '/network/rit/home/ro553136/orrison/data/reconstructions/sstforc_Had_OI/'\n",
    "fh_hadipo = xr.open_dataset(pth_hadsst + 'HADSSTOI_ipoinds_1880_2000.nc', decode_times= True)\n",
    "ipo_index_had = fh_hadipo.tpi_index_11yr\n",
    "\n",
    "#------------------- ENSO -------------------\n",
    "pth_had = '/network/rit/home/ro553136/orrison/data/reconstructions/sstforc_Had_OI/'\n",
    "fh_ens = xr.open_dataset(pth_had + \"HADOISST_N34_dt_1880_2000.nc\")\n",
    "n34ind_had = fh_ens.n34ind_had\n",
    "\n",
    "#------------------- Capture only the djf season -------------------\n",
    "n34ind_djf_had = [np.nanmean(n34ind_had[11+i:14+i],axis=0) for i in range(0,1440,12)]\n",
    "n34ind_djf_had_xr = xr.DataArray(n34ind_djf_had, \n",
    "    coords={'time': icam_yrs}, \n",
    "    dims=[\"time\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf5169",
   "metadata": {},
   "source": [
    "## Model Correlations (IPO/ENSO,x), detrended variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09997abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_ipo = []\n",
    "hatch_ipo = []\n",
    "for var in vars_all_11yr[:-1]:\n",
    "        # build array\n",
    "    cc, p = (xr.DataArray(np.zeros([len(lat), len(lon)]), coords={\"latitude\":lat,\"longitude\":lon}, dims=[\"lat\",\"lon\"],name=var.name)), (xr.DataArray(np.zeros([len(lat), len(lon)]), coords={\"lat\":lat,\"lon\":lon}, dims=[\"lat\",\"lon\"],name=var.name)) \n",
    "    for x in range(len(lat)):\n",
    "        for y in range(len(lon)):\n",
    "                # calculate statistical significance and p value with adjusted df\n",
    "            cc[x,y], p[x,y] = t_p_test(ipo_index_had, var[:,x,y], 11)\n",
    "    cc_ipo.append(cc)\n",
    "    \n",
    "        # create hatching for stippling over statistical significance\n",
    "    [m,n] = np.where(p < 0.05)\n",
    "    hatch=np.zeros(cc.shape)\n",
    "    hatch[m, n] = 1\n",
    "    hatch_ipo.append(hatch)\n",
    "\n",
    "\n",
    "cc_enso = []\n",
    "hatch_enso = []\n",
    "for var in vars_all_dt[:-1]:\n",
    "        # build array\n",
    "    cc, p = (xr.DataArray(np.zeros([len(lat), len(lon)]), coords={\"latitude\":lat,\"longitude\":lon}, dims=[\"lat\",\"lon\"],name=var.name)), (xr.DataArray(np.zeros([len(lat), len(lon)]), coords={\"lat\":lat,\"lon\":lon}, dims=[\"lat\",\"lon\"],name=var.name)) \n",
    "    for x in range(len(lat)):\n",
    "        for y in range(len(lon)):\n",
    "                # calculate statistical significance and p value with adjusted df\n",
    "            nas = np.isnan(var[:,x,y])\n",
    "            cc[x,y], p[x,y] = pearsonr(n34ind_djf_had_xr[~nas], var[:,x,y][~nas])\n",
    "    cc_enso.append(cc)\n",
    "        # create hatching for stippling over statistical significance\n",
    "    [m,n] = np.where(p < 0.05)\n",
    "    hatch=np.zeros(cc.shape)\n",
    "    hatch[m, n] = 1\n",
    "    hatch_enso.append(hatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0c38f",
   "metadata": {},
   "source": [
    "## Proxy correlations (TPI/ENSO, x)\n",
    "For 'Pumacocha', 'UTU', 'Cuy', 'Bol', use adjusted degrees of freedom due to data smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb82ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prox_r_ipo = {}\n",
    "prox_p_ipo = {}\n",
    "for r in recs[:-4]:\n",
    "    prox_r_ipo[r] = t_p_test(ipo_index_had,abs_prox_over_ipo[r],1)[0]\n",
    "    prox_p_ipo[r] = t_p_test(ipo_index_had,abs_prox_over_ipo[r],1)[1]\n",
    "for r in recs[-4:]:\n",
    "    prox_r_ipo[r] = t_p_test(ipo_index_had,abs_prox_over_ipo[r],11)[0]\n",
    "    prox_p_ipo[r] = t_p_test(ipo_index_had,abs_prox_over_ipo[r],11)[1]\n",
    "    \n",
    "prox_r_enso = {}\n",
    "prox_p_enso = {}\n",
    "for r in recs_enso:\n",
    "    if r == 'Pumacocha': continue\n",
    "    else:\n",
    "#         print(r)\n",
    "        prox_r_enso[r] = pearsonr(n34ind_djf_had_xr,abs_prox_over_enso[r])[0]\n",
    "#         print(prox_r_enso[r])\n",
    "        prox_p_enso[r] = pearsonr(n34ind_djf_had_xr,abs_prox_over_enso[r])[1]\n",
    "#         print(prox_p_enso[r])\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d73209",
   "metadata": {},
   "source": [
    "# Correlations (TPI/ENSO, gpcc @ proxy locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4e6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpcc_r_ipo = {}\n",
    "gpcc_p_ipo = {}\n",
    "for i,r in enumerate(recs):\n",
    "    lati = geo_idx(lat_pts[i],gpcc_lat)\n",
    "    loni = geo_idx(lon_pts[i],gpcc_lon)\n",
    "    gpcc_r_ipo[r] = t_p_test(ipo_index_had.sel(time=yrs),vars_all_11yr[3][:,lati,loni],11)[0]\n",
    "    gpcc_p_ipo[r] = t_p_test(ipo_index_had.sel(time=yrs),vars_all_11yr[3][:,lati,loni],11)[1]\n",
    "    \n",
    "gpcc_r_enso = {}\n",
    "gpcc_p_enso = {}\n",
    "for i,r in enumerate(recs_enso):\n",
    "    lati = geo_idx(lat_pts_enso[i],gpcc_lat)\n",
    "    loni = geo_idx(lon_pts_enso[i],gpcc_lon)\n",
    "    gpcc_r_enso[r] = pearsonr(n34ind_djf_had_xr.sel(time=yrs),vars_all_dt[3][:,lati,loni])[0]\n",
    "    gpcc_p_enso[r] = pearsonr(n34ind_djf_had_xr.sel(time=yrs),vars_all_dt[3][:,lati,loni])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6408d",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3df45",
   "metadata": {},
   "source": [
    "# precipitation weighted d18o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFSET proxy correlations\n",
    "cmap = ['PuOr_r', 'BrBG_r', 'RdBu']\n",
    "plt.rc('font', family='sans-serif')\n",
    "plt.rcParams.update({'font.sans-serif':'Arial'})\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "# ------\n",
    "# Set up plotting framework: \n",
    "# ------     \n",
    "fig, axs = pplt.subplots(ncols=2, nrows=3, axwidth=4, share=3, proj='cyl', dpi=300)\n",
    "# fig, axs = pplt.subplots(nrows=1, ncols=1, proj=('cyl'),figsize=(8,5), dpi=300)\n",
    "\n",
    "axs.format(land=False, labels=False, lonlines=20, latlines=10, coast=True, borders = True,\n",
    "           gridminor=True, abc='a', abcloc='l',\n",
    "           collabels=['IPO ','ENSO'],\n",
    "           rowlabels=[r'$\\omega$ [500 hPa]  ',r'$\\delta^{18}O$  ','precipitation  '],\n",
    ")\n",
    "axs.format(lonlim=(260,330), latlim=(-35,10), labels=True)\n",
    "\n",
    "# -------------------------- PLOTTING -------------------------- \n",
    "\n",
    "levs = np.arange(-1.0,1.1,0.2)\n",
    "skip_i = 5 #IPO: doesn't work: 4\n",
    "skip_e = 6 #ENSO: doesn't work: 2, 3, 5, 10, 20 #works: 4\n",
    "# -------------------------- IPO\n",
    "for n in range(3):\n",
    "#     axs[n,:].set_title('')\n",
    "    im = axs[n,0].contourf(lon[::skip_i], lat[::skip_i], cc_ipo[n][::skip_i,::skip_i],levels=levs,cmap=cmap[n])\n",
    "    hatch = axs[n,0].contourf(lon,lat,hatch_ipo[n], hatches=['', '..'],alpha=0.0)\n",
    "    if n == 1: # Plotting proxy isotope correlations\n",
    "        for i,r in enumerate(recs):\n",
    "            if i == 0: #0'P00H1'\n",
    "                if prox_p_ipo[r] < 0.05:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]-1,c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]-1,s=50, marker='+', colors='black', \n",
    "                                            zorder=3,transform=ccrs.PlateCarree())\n",
    "                else:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]-1,c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "            elif i == 8: #8'Pumacocha' \n",
    "                if prox_p_ipo[r] < 0.05:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]+1,c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]+1,s=50, marker='+', colors='black', \n",
    "                                            zorder=3,transform=ccrs.PlateCarree())\n",
    "                else:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i]+1,c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                \n",
    "            elif i == 2: # 2'CR1'  \n",
    "                if prox_p_ipo[r] < 0.05:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i]+1,lat_pts[i],c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                    prox = axs[n,0].scatter(lon_pts[i]+1,lat_pts[i],s=50, marker='+', colors='black', \n",
    "                                            zorder=3,transform=ccrs.PlateCarree())\n",
    "                else:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i]+1,lat_pts[i],c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                \n",
    "            else: #1'JAR4', 3'SBE3', 4'DV2+TR5+LD12', 5'PIM4', 6'MFZ', 7'Quelccaya', 9'UTU', 10'Cuy', 11'Bol'\n",
    "                if prox_p_ipo[r] < 0.05:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i],c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i],s=50, marker='+', colors='black', \n",
    "                                            zorder=3,transform=ccrs.PlateCarree())\n",
    "                else:\n",
    "                    prox = axs[n,0].scatter(lon_pts[i],lat_pts[i],c=prox_r_ipo[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "    if n == 2: # Plotting station precipitation correlations\n",
    "        for i,r in enumerate(recs):\n",
    "            lati = geo_idx(lat_pts[i],gpcc_lat)\n",
    "            loni = geo_idx(lon_pts[i],gpcc_lon)\n",
    "            if gpcc_p_ipo[r] < 0.05:\n",
    "                pob = axs[n,0].scatter(gpcc_lon[loni]-1,gpcc_lat[lati],c=gpcc_r_ipo[r], s=125,\n",
    "                                  cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                  edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                pob = axs[n,0].scatter(gpcc_lon[loni]-1,gpcc_lat[lati],s=50, marker='+', colors='black', \n",
    "                                        zorder=3,transform=ccrs.PlateCarree())\n",
    "            else:\n",
    "                pob = axs[n,0].scatter(gpcc_lon[loni]-1,gpcc_lat[lati],c=gpcc_r_ipo[r], s=125,\n",
    "                                  cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                  edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                \n",
    "# -------------------------- ENSO\n",
    "    im = axs[n,1].contourf(lon[::skip_e], lat[::skip_e], cc_enso[n][::skip_e,::skip_e],levels=levs,cmap=cmap[n])\n",
    "    hatch = axs[n,1].contourf(lon,lat,hatch_enso[n], hatches=['', '..'],alpha=0.0)\n",
    "    if n == 1: # Plotting isotope correlations\n",
    "        for i,r in enumerate(recs_enso):\n",
    "            if r == 'Pumacocha': continue\n",
    "            else:\n",
    "                if prox_p_enso[r] < 0.05:\n",
    "                    prox = axs[n,1].scatter(lon_pts_enso[i],lat_pts_enso[i],c=prox_r_enso[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                    prox = axs[n,1].scatter(lon_pts_enso[i],lat_pts_enso[i],s=50, marker='+', colors='black', \n",
    "                                            zorder=3,transform=ccrs.PlateCarree())\n",
    "                else:\n",
    "                    prox = axs[n,1].scatter(lon_pts_enso[i],lat_pts_enso[i],c=prox_r_enso[r], s=125,\n",
    "                                      cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                      edgecolors='black', transform=ccrs.PlateCarree())    \n",
    "    if n == 2: # plotting precipitation station correlations\n",
    "        for i,r in enumerate(recs_enso):\n",
    "                if r == 'Pumacocha': continue\n",
    "                else:\n",
    "                    lati = geo_idx(lat_pts_enso[i],gpcc_lat)\n",
    "                    loni = geo_idx(lon_pts_enso[i],gpcc_lon)\n",
    "                    if gpcc_p_enso[r] < 0.05:\n",
    "                        pob = axs[n,1].scatter(gpcc_lon[loni],gpcc_lat[lati],c=gpcc_r_enso[r], s=125,\n",
    "                                          cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                          edgecolors='black', transform=ccrs.PlateCarree())\n",
    "                        pob = axs[n,1].scatter(gpcc_lon[loni],gpcc_lat[lati],s=50, marker='+', colors='black', \n",
    "                                                zorder=3,transform=ccrs.PlateCarree())\n",
    "                    else:\n",
    "                        pob = axs[n,1].scatter(gpcc_lon[loni],gpcc_lat[lati],c=gpcc_r_enso[r], s=125,\n",
    "                                          cmap=cmap[n], vmin=-1.0, vmax=1.0,\n",
    "                                          edgecolors='black', transform=ccrs.PlateCarree())\n",
    "# -------------------------- Add colorbar\n",
    "    if n == 1:\n",
    "        axs[n,1].colorbar(im, loc='r', label='correlation coefficient', tickloc='right',length=1.0)\n",
    "    else:\n",
    "        axs[n,1].colorbar(im, loc='r', label='', tickloc='right',length=1.0)\n",
    "    \n",
    "# -------------------------- Add patches for highlighting regions.\n",
    "    \n",
    "w500_rec = axs[0,0].add_patch(patches.Rectangle(xy=[295, -5], width=25, height=15,\n",
    "                            facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2., \n",
    "                            transform=ccrs.PlateCarree()))\n",
    "w500_rec = axs[0,1].add_patch(patches.Rectangle(xy=[295, -5], width=25, height=15,\n",
    "                            facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2.,\n",
    "                            transform=ccrs.PlateCarree()))\n",
    "\n",
    "d18_rec = axs[1,0].add_patch(patches.Rectangle(xy=[285, -15], width=15, height=15,\n",
    "                                facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2., \n",
    "                                transform=ccrs.PlateCarree()))\n",
    "d18_rec = axs[1,1].add_patch(patches.Rectangle(xy=[285, -15], width=15, height=15,\n",
    "                                facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2., \n",
    "                                transform=ccrs.PlateCarree()))\n",
    "\n",
    "p_rec = axs[2,0].add_patch(patches.Rectangle(xy=[285, -15], width=15, height=15,\n",
    "                                facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2., \n",
    "                                transform=ccrs.PlateCarree()))\n",
    "p_rec = axs[2,1].add_patch(patches.Rectangle(xy=[285, -15], width=15, height=15,\n",
    "                                facecolor='none', linestyle = 'dashed', edgecolor='k', lw=2., \n",
    "                                transform=ccrs.PlateCarree()))\n",
    "\n",
    "plt.savefig('/network/rit/lab/vuillelab_rit/orrison/Plots/Pac_SAm/ipo_enso_corr/Fig4_IPO_ENSOdt_w_d18opwt_precip_pobs_corr.jpg', format='JPEG',dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8337dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
